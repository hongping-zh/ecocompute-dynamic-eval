# ‚ö° EcoCompute Dynamic Eval

[![Paper](https://img.shields.io/badge/Paper-Draft-b31b1b.svg)](https://github.com/hongping-zh/ecocompute-dynamic-eval)
[![Dashboard](https://img.shields.io/badge/Dashboard-Live-brightgreen.svg)](https://hongping-zh.github.io/ecocompute-dynamic-eval/)
[![Metadata](https://img.shields.io/badge/Metadata-Complete-blue.svg)](https://github.com/hongping-zh/ecocompute-dynamic-eval/tree/main/metadata)
[![Reproducible](https://img.shields.io/badge/Reproducible-‚úì-success.svg)](https://github.com/hongping-zh/ecocompute-dynamic-eval/tree/main/metadata)
[![Measurements](https://img.shields.io/badge/Measurements-23-orange.svg)](https://github.com/hongping-zh/ecocompute-dynamic-eval/tree/main/metadata/COMPLETE_DATASET_MEMO.md)
[![Data Quality](https://img.shields.io/badge/CV-<2%25-success.svg)](https://github.com/hongping-zh/ecocompute-dynamic-eval/tree/main/metadata)

> **Breakthrough Finding**: bitsandbytes INT8 increases energy by 17-33% due to mixed-precision decomposition. Disabling this pathway recovers **+79% throughput** and **‚àí36% energy**, achieving **5.5% energy savings** vs FP16.

Compare AI models by **Accuracy √ó Cost √ó Carbon** ‚Äî RTX 5090 benchmarks reveal that 4-bit quantization wastes energy on small models.

---

## üèÜ Key Discoveries

### 1. bitsandbytes INT8 Paradox
**Default INT8 is the worst choice for energy efficiency** across all tested models.

| Model | Default INT8 vs FP16 | Pure INT8 vs FP16 | Improvement |
|-------|---------------------|-------------------|-------------|
| Yi-1.5-6B | **+32.7%** ‚ö†Ô∏è | **‚àí3.1%** ‚úÖ | **‚àí34.2%** |
| Mistral-7B | **+30.7%** ‚ö†Ô∏è | **‚àí7.9%** ‚úÖ | **‚àí36.9%** |
| **Average** | **+31.7%** ‚ö†Ô∏è | **‚àí5.5%** ‚úÖ | **‚àí35.6%** |

### 2. Root Cause Identified
Mixed-precision decomposition (INT8‚ÜîFP16 conversion overhead) is the bottleneck, not INT8 itself.

**Evidence**: Disabling decomposition (`llm_int8_threshold=0.0`) recovers:
- **+79% throughput** on average (Yi: +80.9%, Mistral: +77.8%)
- **‚àí36% energy** on average (Yi: -34.2%, Mistral: -36.9%)

### 3. NF4 Crossover Behavior
Energy savings for models ‚â•6B, penalty for <5B.

| Model Size | NF4 vs FP16 | Architecture |
|------------|-------------|--------------|
| 1.1B-3B | **+11.7% to +29.4%** ‚ö†Ô∏è | RTX 5090 Blackwell |
| 6B-7B | **‚àí8.1% to ‚àí34.5%** ‚úÖ | RTX 4090D Ada Lovelace |

### 4. Practical Solution
Set `llm_int8_threshold=0.0` to avoid 30-35% energy penalty. Validate accuracy separately.

---

## üìä Research Quality Standards

This benchmark follows rigorous reproducibility standards:

![Data Quality](https://img.shields.io/badge/Data%20Quality-CV%20%3C%202%25-brightgreen?style=for-the-badge)
![Measurements](https://img.shields.io/badge/Measurements-23-blue?style=for-the-badge)
![Reproducible](https://img.shields.io/badge/Reproducible-‚úì-success?style=for-the-badge)

- ‚úÖ **23 measurements** across 2 GPU architectures (RTX 5090 Blackwell, RTX 4090D Ada Lovelace)
- ‚úÖ **Complete metadata**: Hardware specs, software versions, model commits, quantization configs
- ‚úÖ **High precision**: Coefficient of Variation < 2% (n=10 per configuration)
- ‚úÖ **Causal analysis**: Ablation experiments to isolate root causes
- ‚úÖ **Multi-model validation**: Consistent results across Yi-1.5-6B and Mistral-7B
- ‚úÖ **Open data**: All raw data, scripts, and provenance publicly available

üìÅ **[View Complete Metadata ‚Üí](https://github.com/hongping-zh/ecocompute-dynamic-eval/tree/main/metadata)**

---

## üî¨ Reproducibility Artifacts

All metadata required to reproduce this research is available in the [`metadata/`](https://github.com/hongping-zh/ecocompute-dynamic-eval/tree/main/metadata) directory:

| File | Description | Size | Status |
|------|-------------|------|--------|
| [`rtx5090_metadata.json`](metadata/rtx5090_metadata.json) | RTX 5090 (Blackwell) complete environment | 8 KB | ‚úÖ |
| [`rtx4090d_metadata.json`](metadata/rtx4090d_metadata.json) | RTX 4090D (Ada Lovelace) complete environment | 9 KB | ‚úÖ |
| [`pure_int8_metadata.json`](metadata/pure_int8_metadata.json) | Pure INT8 ablation experiment (Yi-6B + Mistral-7B) | 13 KB | ‚úÖ |
| [`COMPLETE_DATASET_MEMO.md`](metadata/COMPLETE_DATASET_MEMO.md) | Full dataset documentation (23 measurements) | 45 KB | ‚úÖ |

### What's Included

Each metadata file contains:
- **Hardware specifications**: GPU model, architecture, VRAM, Tensor Cores, TDP
- **Software versions**: PyTorch, CUDA, transformers, bitsandbytes (exact versions)
- **Model versions**: HuggingFace paths and commit hashes
- **Quantization configurations**: Complete code snippets for FP16, NF4, INT8 (default), INT8 (pure)
- **Measurement protocol**: Sampling rate (10 Hz), iterations (n=10), prompts, generation settings
- **Data quality metrics**: Coefficient of Variation, sample size, total duration
- **Known issues**: Documented problems and resolutions

### Reproduction Commands

```bash
# RTX 5090 reproduction
python energy_benchmark.py --gpu rtx5090 --models all --configs fp16,nf4

# RTX 4090D standard reproduction
python energy_benchmark.py --gpu rtx4090d --models all --configs fp16,nf4,int8

# RTX 4090D pure INT8 reproduction
python test_pure_int8.py --model 01-ai/Yi-1.5-6B-Chat
python test_pure_int8_mistral.py --model mistralai/Mistral-7B-Instruct-v0.3
```

---

## üìà Data Quality

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Total measurements | **23** | 8 RTX 5090 + 12 RTX 4090D + 2 Pure INT8 + 1 Mistral Pure INT8 |
| Coefficient of Variation | **0.3-1.7%** | Excellent reproducibility |
| Sample size per config | **n=10** | Sufficient for statistical power |
| Total benchmark time | **~15 hours** | Comprehensive coverage |
| Cross-model consistency | **¬±3.5%** | Very high |

---

## üéØ Impact

This research prevents a potential industry-wide mistake:

### Without This Work
- ‚ùå Industry conclusion: "INT8 is bad for energy, avoid it"
- ‚ùå NVIDIA's INT8 Tensor Cores underutilized
- ‚ùå Missed opportunity for energy savings
- ‚ùå 30-35% energy waste in production deployments

### With This Work
- ‚úÖ Industry conclusion: "bitsandbytes INT8 is bad due to decomposition; use TensorRT/GPTQ or set threshold=0.0"
- ‚úÖ Correct understanding of INT8's value
- ‚úÖ Energy savings realized in production
- ‚úÖ Clear actionable guidance for practitioners

---

## üöÄ Quick Start

### View Dashboard
Visit the [live dashboard](https://hongping-zh.github.io/ecocompute-dynamic-eval/) to explore interactive visualizations of all benchmark results.

### Explore Metadata
Browse the [`metadata/`](metadata/) directory for complete reproducibility artifacts.

### Run Benchmarks
Clone the repository and follow reproduction commands above.

---

## üìö Citation

If you use this data or methodology, please cite:

```bibtex
@techreport{zhang2026quantization,
  title={Energy Efficiency of Quantized Large Language Model Inference: 
         Evidence for Quantization Efficiency Paradoxes},
  author={Zhang, Hongping},
  year={2026},
  institution={Independent Research},
  url={https://github.com/hongping-zh/ecocompute-dynamic-eval},
  note={23 measurements across RTX 5090 Blackwell and RTX 4090D Ada Lovelace}
}
```

---

## üîó Links

- üìä **[Live Dashboard](https://hongping-zh.github.io/ecocompute-dynamic-eval/)**: Interactive visualization
- üìÑ **[Paper (Draft)](https://github.com/hongping-zh/ecocompute-dynamic-eval)**: Full technical report
- üìÅ **[Metadata](https://github.com/hongping-zh/ecocompute-dynamic-eval/tree/main/metadata)**: Complete reproducibility artifacts
- üíª **[Code](https://github.com/hongping-zh/ecocompute-ai)**: Benchmark scripts and raw data

---

## ü§ù Contributing

Contributions are welcome! If you:
- Run benchmarks on additional GPUs (A100, H100, etc.)
- Test alternative quantization methods (GPTQ, TensorRT, llama.cpp)
- Measure accuracy impact of pure INT8
- Find issues or have suggestions

Please open an issue or submit a pull request.

---

## üìß Contact

- **Author**: Hongping Zhang
- **Email**: zhanghongping1982@gmail.com
- **GitHub**: [@hongping-zh](https://github.com/hongping-zh)

---

## üìù License

MIT License - See [LICENSE](LICENSE) for details.

---

## üôè Acknowledgments

- **AutoDL** for providing GPU cloud infrastructure
- **HuggingFace** for model hosting and transformers library
- **bitsandbytes** team for quantization library (and inspiring this research!)
- **Open source community** for tools and support

---

*"Measure, don't assume. Reproduce, don't trust. Share, don't hoard."*
